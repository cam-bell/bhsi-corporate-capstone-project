#!/usr/bin/env python3
"""
Simple RSS test - verify feeds work without full app dependencies
"""

import asyncio
import aiohttp
import feedparser
from datetime import datetime


def _parse_date(date_str: str) -> str:
    """Parse various date formats to ISO format"""
    try:
        if "T" in date_str:
            return date_str
        else:
            formats = [
                "%a, %d %b %Y %H:%M:%S %z",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d"
            ]
            for fmt in formats:
                try:
                    parsed = datetime.strptime(date_str, fmt)
                    return parsed.isoformat() + "Z"
                except ValueError:
                    continue
            return datetime.now().isoformat() + "Z"
    except Exception:
        return datetime.now().isoformat() + "Z"


async def test_elpais_feeds():
    """Test El Pa√≠s RSS feeds directly"""
    print("\n" + "="*50)
    print("TESTING EL PA√çS RSS FEEDS")
    print("="*50)
    
    feeds = [
        {
            "category": "portada",
            "url": "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada"
        },
        {
            "category": "economia",
            "url": ("https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/"
                   "section/economia/portada")
        },
        {
            "category": "negocios",
            "url": ("https://feeds.elpais.com/mrss-s/list/ep/site/elpais.com/"
                   "section/economia/subsection/negocios")
        }
    ]
    
    headers = {
        "User-Agent": "BHSI-Risk-Assessment/1.0",
        "Accept": "application/rss+xml, application/xml",
        "Accept-Language": "es-ES,es;q=0.9,en;q=0.8"
    }
    
    timeout = aiohttp.ClientTimeout(total=10)
    
    async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
        for feed in feeds:
            try:
                print(f"Testing feed: {feed['category']}")
                
                async with session.get(feed["url"]) as response:
                    if response.status == 200:
                        content = await response.text()
                        parsed_feed = feedparser.parse(content)
                        
                        print(f"  ‚úÖ Status: {response.status}")
                        print(f"  üì∞ Entries: {len(parsed_feed.entries)}")
                        
                        if parsed_feed.entries:
                            # Show first entry
                            first_entry = parsed_feed.entries[0]
                            print(f"  üìÑ Sample: {first_entry.get('title', 'No title')[:50]}...")
                            print(f"  üìÖ Date: {first_entry.get('published', 'No date')}")
                            print(f"  üîó URL: {first_entry.get('link', 'No link')}")
                        print()
                        
                    else:
                        print(f"  ‚ùå Status: {response.status}")
                        print()
                        
            except Exception as e:
                print(f"  ‚ùå Error: {e}")
                print()


async def test_expansion_feeds():
    """Test Expansi√≥n RSS feeds directly"""
    print("\n" + "="*50)
    print("TESTING EXPANSI√ìN RSS FEEDS")
    print("="*50)
    
    feeds = [
        {
            "category": "empresas-nombramientos",
            "url": "https://e00-expansion.uecdn.es/rss/empresas/nombramientos.xml"
        },
        {
            "category": "economia-portada",
            "url": "https://e00-expansion.uecdn.es/rss/economia.xml"
        },
        {
            "category": "juridico-sentencias",
            "url": "https://e00-expansion.uecdn.es/rss/juridicosentencias.xml"
        }
    ]
    
    headers = {
        "User-Agent": "BHSI-Risk-Assessment/1.0",
        "Accept": "application/rss+xml, application/xml",
        "Accept-Language": "es-ES,es;q=0.9,en;q=0.8"
    }
    
    timeout = aiohttp.ClientTimeout(total=10)
    
    async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
        for feed in feeds:
            try:
                print(f"Testing feed: {feed['category']}")
                
                async with session.get(feed["url"]) as response:
                    if response.status == 200:
                        content = await response.text()
                        parsed_feed = feedparser.parse(content)
                        
                        print(f"  ‚úÖ Status: {response.status}")
                        print(f"  üì∞ Entries: {len(parsed_feed.entries)}")
                        
                        if parsed_feed.entries:
                            # Show first entry
                            first_entry = parsed_feed.entries[0]
                            print(f"  üìÑ Sample: {first_entry.get('title', 'No title')[:50]}...")
                            print(f"  üìÖ Date: {first_entry.get('published', 'No date')}")
                            print(f"  üîó URL: {first_entry.get('link', 'No link')}")
                        print()
                        
                    else:
                        print(f"  ‚ùå Status: {response.status}")
                        print()
                        
            except Exception as e:
                print(f"  ‚ùå Error: {e}")
                print()


async def test_search_functionality():
    """Test search functionality with sample data"""
    print("\n" + "="*50)
    print("TESTING SEARCH FUNCTIONALITY")
    print("="*50)
    
    # Simulate search results
    test_query = "Banco Santander"
    query_terms = test_query.lower().split()
    
    print(f"Search query: {test_query}")
    print(f"Query terms: {query_terms}")
    
    # Test with sample article data
    sample_articles = [
        {
            "title": "Banco Santander anuncia nuevos beneficios para clientes",
            "description": "El banco espa√±ol presenta nuevas ofertas en productos financieros",
            "source": "El Pa√≠s",
            "category": "economia"
        },
        {
            "title": "Expansi√≥n del mercado financiero en Espa√±a",
            "description": "An√°lisis del crecimiento del sector bancario",
            "source": "Expansi√≥n",
            "category": "empresas"
        },
        {
            "title": "Nuevas regulaciones afectan al sector bancario",
            "description": "Cambios en la normativa financiera espa√±ola",
            "source": "El Pa√≠s",
            "category": "negocios"
        }
    ]
    
    matches = []
    for article in sample_articles:
        title_lower = article["title"].lower()
        desc_lower = article["description"].lower()
        
        if any(term in title_lower or term in desc_lower for term in query_terms):
            matches.append(article)
    
    print(f"Found {len(matches)} matching articles:")
    for i, match in enumerate(matches, 1):
        print(f"  {i}. {match['title']}")
        print(f"     Source: {match['source']} - {match['category']}")
        print()


async def main():
    """Run all tests"""
    print("üöÄ SIMPLE RSS FEED TESTING")
    print("Testing RSS feeds without full app dependencies")
    
    await test_elpais_feeds()
    await test_expansion_feeds()
    await test_search_functionality()
    
    print("\n" + "="*50)
    print("TESTING COMPLETE")
    print("="*50)
    print("\n‚úÖ RSS feeds are working correctly!")
    print("üìù Next steps:")
    print("   1. Integrate agents into streamlined_orchestrator.py")
    print("   2. Update search endpoints to include new sources")
    print("   3. Test with full classification pipeline")


if __name__ == "__main__":
    asyncio.run(main()) 